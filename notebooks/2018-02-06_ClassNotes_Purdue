# Fourth Tuesday
## Time for some statistical material, at last 

### Largely a vocabulary lesson, aiming toward a test 
*** SIDE NOTE:  to print .md files in a "pretty" format, change url from "github" to "gitprint" ***

- x,y,z for cartesian points vs. X and Y for arbitrary variables
NOTATIONS (formal and informal)
  - functions and calculus (continuous)
      - f(x) and df/dx
  - discrete math
      - {xi} and summation of xi from i=1 to i=N
      - 1/N times the summation of Xi.Yi from i = 1 to i=N
  - matrix notation
      -[ X1 X2 ... Xn ], [Y1 Y2 ... Yn ]
      - X'Y = Y'X
  - informal "prime and bar"
      - [XY] and [X*Y*] etc
  - psuedo-code
      - cov(X,Y)
  - real code

*** STATISTICS ***
Given a vector, X, with values equal to:

X = {Xi} = {5,3, -2, 11,0, -7, 4}

Virtues of a statistic:
  - simple
  - robust (to outliers)
  - efficient (converges with large N)
  - unbiased

"central value"
  mean = 2
  median = 3

"spread"
  range =  {-7, 11}
  mean absolute deviation = mean(abs(X'))
    -also known as "L1 norm"
  variance = mean((X')^2)
    -also known as "L2 norm"
    -side-note:  "L-infinity norm" is a fancy way of saying the maximum
  stdev = sqrt(mean((X')^2))
  RMS = sqrt(mean(X^2))
  
"skewness" (assymetry)
  = mean((X')^3)
  
*** PROBABILITY ***

Maybe the sample, X, is representative of an infinite population...
Let's infer characteristics of this population from X

  Probability Distribution/Density Function (PDF) = P(X)
    -Statistics of samples function as *estimators* of P(x)'s characteristics
    -we know integral(P(X)dX) from -inf to inf = 1
    -mean = integral((P(X) X dX)) from -inf to inf
      -1st moment
    -variance = integral(P(X') X'^2 dX)
      -2nd moment
    -skew (same but X'^3)
      -3rd moment
      


- Probability
  - Frequentist
  - Bayesian 
    - prior information, data, posterior 
  
- Random variable (vs. deterministic variable) 

- Event, Realization

Population, Sample

Biased sample

Distribution vs. density (PDF) 

Likelihood 

CDF

Moments and other summaries of distributions
  - Central value
    - mean (first moment) and expected value
    - median
    - mode (likelihood peak)
  - Spread (dispersion, variation) 
    - range
    - L1 norm (mean absolute deviation) 
    - L2 norm (variance) var(x) 
    - stdev
    - skew and kurtosis
   Spread (variance) of a function or combination of variables 
   
Central Limit Theorem and "Normal" distribution
  - and its undersampled cousin the Student's t distribution 
  - and chi-squared distribution 
  - The F distribution, the ratio of sample variances from a Normal population. 
    - all depend on number of samples, and level out for n > 20 say

Standardized variables 
  - Z score: how many sigma from the mean
  - converts to a "p value" based on the Normal distribution
    - one or two tailed
    - t test, for small samples
  
Multivariate distributions
  - joint
  - marginal
  - conditional
  
Scatter plots and covariance 
  - "Independent" variables
    - linearly independent
    - orthogonal, uncorrelated

Contingency tables 

------- Statistics 

Estimation, estimator, statistic 

Parametric vs. nonparametric 

Histogram, kernel density estimator -- "relative frequency"

consistent vs. inconsistent statistic (does sample size improve it) 
  - efficient: converges quickly 

unbiased vs. biased statistic (often, maximum likelihood) 

robust (to outliers) 
  - like rank 

"Ideally, it is best to use consistent, unbiased and efficient estimators"

mean squared error MSE, and RMS 

Hypothesis testing 
  - Null hypothesis
  - Occam's Razor and parsimony 
  - false positive, false negativ 
  
1) Specify a test statistic
2) Specify a level of significance 
3) Determine the limiting value(s) of the test statistic that correspond 
  - If the test statistic does not fall within the critical region, cannot reject the null hypothesis 

Degrees of freedom (d.f. or ν) is an important concept in statistics as well as mathe- matics, modeling and physics. In statistics, degrees of freedom represent the number of independent observations used to estimate a population parameter. Effectively, they may be thought of as representing the number of independent observations in the sample (n) minus the number of parameters already estimated from these n observations (np)

Level of Significance 
  - S/N ratio
  
Confidence interval, p-value, and 95% convention 
  - standard error of the mean 
  
The t test or Student’s t test is used most frequently to (1) compare a sample mean to a hypothetical mean or (2) compare two sample means, when sample sizes are small (say, n ≤ 25). For the former, the test is performed as done previously with the Z statistic
  
Monte carlo approaches 
  - Resampling, bootstrap, jackkinfe

5 percent significanc: xkcd jellybeans 








